{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6252f42-364b-45c7-ad3d-b9cc72873f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import hail as hl\n",
    "\n",
    "from hail.plot import show\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "hl.plot.output_notebook()\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679e50f-9502-453d-85ba-3b38bbae1e3a",
   "metadata": {},
   "source": [
    "### Spark instance: Read data from Minio bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ebe51c-4ab9-451e-a32f-226e5e477c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_name = f\"logs/hail-{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}.log\"\n",
    "# run spark\n",
    "spark_conf = SparkConf().setAppName(\"hail-test\")\n",
    "# .setMaster(\"spark://spark-master:7077\")\n",
    "spark_conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://lifemap-minio:9000/\")\n",
    "spark_conf.set(\"spark.hadoop.fs.s3a.access.key\", \"root\")\n",
    "spark_conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"passpass\" )\n",
    "spark_conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "spark_conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "spark_conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark_conf.set(\"spark.hadoop.fs.s3a.connection.maximum\", 1024);\n",
    "spark_conf.set(\"spark.hadoop.fs.s3a.threads.max\", 1024);\n",
    "spark_conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc = SparkContext(conf=spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033694e-841d-487e-be80-53ad248a708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create bucket if it does not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5137a-45aa-4d0d-8b42-df50d3c97b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "# S3 configuration\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=\"http://lifemap-minio:9000\",\n",
    "    aws_access_key_id=\"root\",\n",
    "    aws_secret_access_key=\"passpass\",\n",
    ")\n",
    "\n",
    "bucket_name = \"data-hail\"\n",
    "\n",
    "# Check if the bucket exists, if not, create it\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' exists.\")\n",
    "except Exception:\n",
    "    # If the bucket does not exist, create it\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7919cb2-8906-44d2-8750-7c359ce58798",
   "metadata": {},
   "source": [
    "### Init Hail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113ae2b-9754-4e8b-b2d3-2839ac142b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.init(sc=sc, log=log_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e3c63-e86d-46af-97c5-8651867fbc9c",
   "metadata": {},
   "source": [
    "#### Import and matrix table filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60272387-1744-4e83-a428-da2df979f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test read\n",
    "vcf_fn = 'data/1kg.vcf'\n",
    "annotations_fn = 'data/1kg_annotations.txt'\n",
    "mt_fn = 's3://data-hail/1kg.mt'\n",
    "\n",
    "print (f\"Input fn: {vcf_fn}\")\n",
    "print (f\"Matrix table fn: {mt_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f6ab9-715c-4f47-a3bf-cf52e5912bbe",
   "metadata": {},
   "source": [
    "#### Reading vcf with Pandas (N/A if the vcf is stored on s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd9a733-da94-4149-b507-c636ec4087e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_pd = None\n",
    "if \"s3://\" not in vcf_fn: \n",
    "    vcf_pd = pd.read_csv(vcf_fn, sep=\"\\t\", header=109)\n",
    "vcf_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d74bab-ae72-4ebb-a48c-87577ba8b411",
   "metadata": {},
   "source": [
    "### Import VCF to hail as Matrix Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc132d8-384b-49ab-aaf6-4bd5228a0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read a vcf file and write it as matrix table\n",
    "_ = hl.import_vcf(vcf_fn).write(mt_fn, overwrite=True) # assign this to a dummy variable to avoid errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ebad8-b211-480e-88a0-de4d5a560539",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the matrix table from the file and assign it to the mt vaiable\n",
    "mt = hl.read_matrix_table(mt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcab9e-7302-49ee-962a-c58ca3d48054",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary of the table. This shows all matrix table component:\n",
    "# 1) global fields\n",
    "# 2) column fields (they are rows of the column table)\n",
    "# 3) row fields (the columns of the row table)\n",
    "# 3) entry fields (the columns of the entry table)\n",
    "mt.describe()\n",
    "\n",
    "## Counts of samples and variants\n",
    "print (\" Printing row and column fields with different calls\")\n",
    "# count row fields\n",
    "print(mt.count_rows())\n",
    "# same as\n",
    "print (mt.rows().count())\n",
    "\n",
    "# count column fields\n",
    "print(mt.count_cols())\n",
    "# same as\n",
    "print(mt.cols().count())\n",
    "\n",
    "n_variants = mt.count_rows() # This can be done accessing directy the row table with mt.rows().count()\n",
    "n_samples = mt.count_cols()  # This can be done accessing directy the column table with mt.cols().count()\n",
    "\n",
    "print (f\"\\n\\nTable has {n_variants} variants and {n_samples} samples\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5c7fd-1b4b-4281-a2a2-c7f5b7e8143d",
   "metadata": {},
   "source": [
    "#### Row table:\n",
    "Contains all vcf columns except FORMAT and samples. Each row data is constant for the columns (samples) of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccae31-81b3-4dd4-afd8-de014d7a91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.rows().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be07b06-9072-47e1-a4f0-a487595f4a5e",
   "metadata": {},
   "source": [
    "#### Column table\n",
    "Contains sample ids as rows. It can have also columns that represents data fields specific for each sample, so constant for each row of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55519c09-f11e-4099-afe7-63cc106f93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.cols().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0044105-9171-45a4-88f2-bbb18be0626b",
   "metadata": {},
   "source": [
    "#### Entry table.\n",
    "The entry table has a specific value for the pair (variant, sample) for the fieds specified in the vcf format column.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e095bc-f3d6-41d8-acfc-820babfb0768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mt.entries().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abb72b-1c45-4d19-8799-6cc486304fe6",
   "metadata": {},
   "source": [
    "#### Global values.\n",
    "Common values of the matrix table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc999fe-c9cc-400b-9d1d-4f3b3b8aa488",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.globals_table().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b1bdb-3248-47dd-ad21-d0260757f463",
   "metadata": {},
   "source": [
    "#### Getting to know our data\n",
    "\n",
    "In this case the matrix array data structure does not have global fields but has got column and row fields. \n",
    "\n",
    "- row fields: each row specifies a variants and row fields are informations common to that variants (for examples alleles or quality).\n",
    "- column fields: each row is specific for a sample and, in this case, specifies the sample ID\n",
    "- entry fields: they are specific data for a pair (variant, sample), for example the genotype read for that particular sample.\n",
    "- column key: it is the column field used for join operations\n",
    "- row key: it is the row field used for join operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f1364-99e8-4e46-a19d-d68287774fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mt.rows() gets the row table. Using select() without arguments returns the table key columns as specified in the documentation:\n",
    "## Select methods will always preserve the key along that axis; e.g. for Table.select(), the table key will aways be kept. To modify the key, use key_by().\n",
    "mt.rows().select().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc9462-93d6-4b2d-820a-b68f8be6763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same can be done by using the matrix table select_row method directly. \n",
    "mt.select_rows().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ac85e-e345-4651-9cf4-cc938f816a02",
   "metadata": {},
   "source": [
    "#### Show attributes of entry fields. An example with the genotype field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d392ae7-ef97-422d-a78e-aa130cd2e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_expr = mt.GT # Takes the GT entry field for all samples \n",
    "gt_expr.phased.show() # Show the phased attribute of the GT field (It is False for not phased haplotypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f9a5b-9050-449e-a5d6-83dbe0002442",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_expr.ploidy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1257c22-ea80-45b6-a915-65dff49937c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_expr.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd297aad-e608-4ab2-b3b2-ffebc420529c",
   "metadata": {},
   "source": [
    "#### The entries method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a80728-29a7-4966-a430-5af51927fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_structure = mt.entry\n",
    "# To show all entry field names\n",
    "list(entry_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536fc7e-49b7-43c6-a0f7-d95d0a58ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entries method returns a matrix in coordinate table form (It is a hail table). Take a look to the API for typical applications\n",
    "# Rouhgly speaking, entries makes a flattening version of the data structure and allows to perform \n",
    "# grouping on rows and cols simultaneously \n",
    "mt_entries = mt.entries()\n",
    "mt_entries.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe42b7a-7213-4a52-89ad-413915929336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The first n_sample rows deal with the info of the first variant for each sample.\n",
    "## Here first 10 entries. It can be seen from the column locus (it has the same value) and the\n",
    "## s column that changes to specify the sample ID.\n",
    "## The n_samples + 1 row shows the next variant for the first sample ID.\n",
    "mt_entries.show(n_samples + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8e05c-7c44-4e73-9785-ff813c04c34f",
   "metadata": {},
   "source": [
    "#### Showing rows, cols and entry field data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f6564e-90c2-4464-a085-1d83130cf4ba",
   "metadata": {},
   "source": [
    "It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.\n",
    "\n",
    "The rows method can be used to get a table with all the row fields in our MatrixTable.\n",
    "\n",
    "We can use rows along with select to pull out 5 variants. The select method takes either a string refering to a field name in the table, or a Hail Expression. Here, we leave the arguments blank to keep only the row key fields, locus and alleles.\n",
    "\n",
    "Use the show method to display the variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1449d-29ef-482f-9dff-85b1d9a5e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.rows().select().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927bc734-075b-4597-9788-c9684232c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the same with \n",
    "mt.row_key.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62beeb6d-6ff4-49b6-9095-526e35d7c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The matrix table has also the colum dimension with the sample IDS\n",
    "## To peek at the first few sample IDs\n",
    "## s is the ID field\n",
    "\n",
    "mt.s.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d073d46-7cab-48ff-90e2-7d8a529e45ba",
   "metadata": {},
   "source": [
    "To look at the first few genotype calls, we can use entries along with select and take. The **take method collects the first n rows into a list**. Alternatively, we can use the show method, which prints the first n rows to the console in a table format.\n",
    "\n",
    "**Enrty method returns the list of all entry fields sequentially**. If the number of samples is M, taking M+1 entry elements returns the first variant of all samples plus the second variant of the first sample.\n",
    "Try changing take to show in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a40dfc-52ae-4ad0-a4a7-198afb79728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take takes the first 10 entries \n",
    "mt.entry.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9c8f4-afc4-4006-8dcf-32713360bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 10 variants of the first sample\n",
    "mt.entry.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4269f6-5616-4f32-bb7f-6ec75231e2a4",
   "metadata": {},
   "source": [
    "### Hail Table object: Adding column fields to the matrix table starting from a metadata Table (annotations)\n",
    "\n",
    "Metadata of samples (like phenotypes or geographical origin) are saved in a separate txt file imported to Hail as a Table.\n",
    "\n",
    "A Hail MatrixTable can have any number of row fields and column fields for storing data associated with each row and column. Annotations are usually a critical part of any genetic study. **Column fields are where you’ll store information about sample phenotypes, ancestry, sex, and covariates. Row fields can be used to store information like gene membership and functional impact for use in QC or analysis**.\n",
    "\n",
    "In this tutorial, we demonstrate how to take a text file and use it to annotate the columns in a MatrixTable.\n",
    "\n",
    "The file provided contains the sample ID, the population and “super-population” designations, the sample sex, and two simulated phenotypes (one binary, one discrete).\n",
    "\n",
    "This file can be imported into Hail with **import_table**. This function produces a **Table object**. Think of this as a Pandas or R dataframe that isn’t limited by the memory on your machine – behind the scenes, it’s distributed with Spark. **The Table object, like the matrix table object, is immutable**. To interact with it locally as a Python datastructure, you should use the **take** method or transform to a Pandas dataframe.\n",
    "\n",
    "**Table can have global field and row fields**. Global field is common for each element of the table whereas row fields are specific for each row. In this case, each row refers specific attributes of a sample. For example, its ID is specified in the \"Sample\" row field whereas the other fields show its metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1030732-5872-4969-ba6a-17032aea41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_table = (hl.import_table(annotations_fn, impute=True)\n",
    "         .key_by('Sample'))\n",
    "\n",
    "annotation_table.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18151a12-828d-4575-ada3-4e04900dc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57951e46-c273-4de9-94d9-3436701e0b2b",
   "metadata": {},
   "source": [
    "#### Query functions and the Hail Expression Language\n",
    "\n",
    "Hail has a number of useful query functions that can be used for gathering statistics on our dataset. These query functions take Hail Expressions as arguments.\n",
    "\n",
    "We will start by looking at some statistics of the information in our table. The **aggregate** method can be used to aggregate over rows of the table.\n",
    "\n",
    "**counter** is an aggregation function that counts the number of occurrences of each unique element. We can use this to pull out the population distribution by passing in a Hail Expression for the field that we want to count by.\n",
    "\n",
    "The aggregate method is then used to aggregate something in the table across different rows and the aggregate function like counter, stats, etc... are used to specify how and what to aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84858687-a300-4a2b-ad16-025b2d2d58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Population distribution\n",
    "## Here counter counts unique geographycal origin label\n",
    "\n",
    "aggregate_expression = hl.agg.counter(annotation_table.SuperPopulation)\n",
    "pprint(annotation_table.aggregate(aggregate_expression))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a0664-a1aa-466e-9539-f81faf49796e",
   "metadata": {},
   "source": [
    "**stats** is an aggregation function that produces some useful statistics about numeric collections. We can use this to see the distribution of the CaffeineConsumption phenotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec22783-40dc-4a7c-ba9e-6ed59ae15900",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stats perform some statistics on the specified field\n",
    "## Here take stats of the caffeine consumption\n",
    "\n",
    "aggregate_expression = hl.agg.stats(annotation_table.CaffeineConsumption)\n",
    "pprint(annotation_table.aggregate(aggregate_expression))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cba46-da9d-42a5-a06f-97f25d6702c2",
   "metadata": {},
   "source": [
    "The functionality demonstrated in the last few cells isn’t anything especially new: **it’s certainly not difficult to ask these questions with Pandas or R dataframes, or even Unix tools like awk. But Hail can use the same interfaces and query language to analyze collections that are much larger, like the set of variants.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d81a5c-8953-4c6f-b114-b22d14566657",
   "metadata": {},
   "source": [
    "### Join sample annotations with the matrix table\n",
    "\n",
    "Using the **annotate_cols** method is possible to join the annotation table with the MatrixTable containing our dataset.\n",
    " First, we’ll print the existing column schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d701a5-7147-4063-936b-ed7fcf4aea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column table before adding per sample annotation\n",
    "mt.col.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60fff5-90be-438e-bd59-f9bee1b7eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_cols(pheno = annotation_table[mt.s])\n",
    "\n",
    "# After the annotation the columns has a new field pheno,\n",
    "# a struct that contains sample metadata\n",
    "mt.col.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d8346-71ea-4fce-b4cd-8a2bfe6aefdf",
   "metadata": {},
   "source": [
    "Each column \"name\" now specifies the sample ID and its phenotype. Values for each column are still the entry field values (for example Genotyping values (GT))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327cdcb6-c9d0-4a5c-b96a-c99ccda9c1da",
   "metadata": {},
   "source": [
    "Here we calculate the counts of each of the 12 possible unique SNPs (4 choices for the reference base * 3 choices for the alternate base).\n",
    "\n",
    "To do this, we need to get the alternate allele of each variant and then count the occurences of each unique ref/alt pair. This can be done with Hail’s **counter function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7c516-1305-4761-a9e6-c374aec68f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.alleles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db666a-2e47-4158-a9c6-8d7264e7a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_expr = hl.Struct(ref=mt.alleles[0], alt=mt.alleles[1])\n",
    "struct_expr.get('alt').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad738733-9f3a-4350-9996-846492147cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_expr = hl.Struct(ref=mt.alleles[0], alt=mt.alleles[1])\n",
    "snp_counts = mt.aggregate_rows(hl.agg.counter(struct_expr))\n",
    "pprint(snp_counts)\n",
    "\n",
    "# We can list the counts in descending order using Python’s Counter class.\n",
    "from collections import Counter\n",
    "counts = Counter(snp_counts)\n",
    "counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a051048-2e67-43c9-bbfb-80b99bcf7aed",
   "metadata": {},
   "source": [
    "**The same Python, R, and Unix tools could do this work as well, but we’re starting to hit a wall - the latest gnomAD release publishes about 250 million variants, and that won’t fit in memory on a single computer.**\n",
    "\n",
    "What about genotypes? Hail can query the collection of all genotypes in the dataset, and this is getting large even for our tiny dataset. Our 284 samples and 10,000 variants produce 10 million unique genotypes. The gnomAD dataset has about 5 trillion unique genotypes.\n",
    "\n",
    "Hail plotting functions allow Hail fields as arguments, so we can pass in the DP field directly here. If the range and bins arguments are not set, this function will compute the range based on minimum and maximum values of the field and use the default 50 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7657b-3c38-4ac6-aa6b-a056042b1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2454d-c862-4a4b-aa44-837218d89eda",
   "metadata": {},
   "source": [
    "### Quality control\n",
    "\n",
    "QC is where analysts spend most of their time with sequencing datasets. QC is an iterative process, and is different for every project: there is no “push-button” solution for QC. Each time the Broad collects a new group of samples, it finds new batch effects. However, by practicing open science and discussing the QC process and decisions with others, we can establish a set of best practices as a community.\n",
    "\n",
    "QC is entirely based on the ability to understand the properties of a dataset. Hail attempts to make this easier by providing the sample_qc function, which produces a set of useful metrics and stores them in a column field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5425e98-0d63-48b4-b2ef-6382931bf366",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.col.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255d1b0-87ca-4d1e-a213-0cc451dcf8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_qc is a hail genetic method to compute per-sample metrics useful for quality control.\n",
    "mt = hl.sample_qc(mt)\n",
    "mt.col.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e458ef-a374-4037-ba8c-e583df52910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting the QC metrics is a good place to start.\n",
    "\n",
    "p = hl.plot.histogram(mt.sample_qc.call_rate, range=(.88,1), legend='Call Rate')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb13533-e96d-4847-8220-6de6cd16b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.histogram(mt.sample_qc.n_not_called, legend='Not called')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c99a6-ff31-4cde-8ea7-7d7acec11650",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.histogram(mt.sample_qc.gq_stats.mean, range=(10,70), legend='Mean Sample GQ')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed4b8e-61bd-40fd-a136-269fbce00908",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking corralations between the mean value of dp and the call rate\n",
    "p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f13dd-b947-420c-9e2b-a308f677bfaf",
   "metadata": {},
   "source": [
    "#### Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8a553-09e9-49b3-b2da-acdbc107228c",
   "metadata": {},
   "source": [
    "Removing outliers from the dataset will generally improve association results. We can make arbitrary cutoffs and use them to filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90eebb-0e0d-4cfc-bfbf-33ad0e79e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It creates a new matrix table considering samples with the DP mean >= 4 and a call rate >= 0.97\n",
    "## samples that don't satisfy these criteria are removed \n",
    "\n",
    "mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97))\n",
    "print('After filter, %d/284 samples remain.' % mt.count_cols())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c7b1c3-0242-4808-b834-bc39a7526957",
   "metadata": {},
   "source": [
    "Next is genotype QC. It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.\n",
    "\n",
    "In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08493728-d136-4008-b779-745c1514cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = mt.AD[1] / hl.sum(mt.AD)\n",
    "\n",
    "filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |\n",
    "                        (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |\n",
    "                        (mt.GT.is_hom_var() & (ab >= 0.9)))\n",
    "\n",
    "fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab))\n",
    "print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.')\n",
    "mt = mt.filter_entries(filter_condition_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9a82b-2650-4953-9240-4662e8f1dd79",
   "metadata": {},
   "source": [
    "Variant QC computes per per-variant metric useful for quality control. It is a bit more of the same of sample_qc: we can use the variant_qc function to produce a variety of useful statistics, plot them, and filter. This is made at row level beacause they are stats on variants.\n",
    "\n",
    "These statistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_rows method can help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4bb5d8-40bf-41fa-82d6-e8fb6e124b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.variant_qc(mt)\n",
    "mt.row.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d4896-abac-4c46-ae94-dcfc79c0d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.histogram(mt.variant_qc.call_rate, legend='Variant QC call rate')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075f48c-7edc-4dea-b16d-a392c9bff300",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.histogram(mt.variant_qc.dp_stats.mean, legend='Variant QC DP')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7315f777-bc1b-452b-858c-15a700d5e189",
   "metadata": {},
   "source": [
    "## Let’s do a GWAS!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b07509-4460-47d3-b913-9702ad0dfe24",
   "metadata": {},
   "source": [
    "First, we need to restrict to variants that are :\n",
    "\n",
    "- common (we’ll use a cutoff of 1%)\n",
    "- not so far from Hardy-Weinberg equilibrium as to suggest sequencing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb857418-924f-426c-b332-8fb14620f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01) # It takes variants for which the alternate allele has a frequency larger than 1%\n",
    "print('Samples: %d  Variants: %d' % (mt.count_cols(), mt.count_rows()))\n",
    "\n",
    "mt = mt.filter_rows(mt.variant_qc.p_value_hwe > 1e-6) # Hardy-Weinberg equilibrium pvalue cut-off\n",
    "print('Samples: %d  Variants: %d' % (mt.count_cols(), mt.count_rows()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606e349-33af-444c-8747-48505c73c3b9",
   "metadata": {},
   "source": [
    "These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.\n",
    "\n",
    "In Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd29f27-6114-47c8-8843-a6b67bc5d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,\n",
    "                                 x=mt.GT.n_alt_alleles(),\n",
    "                                 covariates=[1.0])\n",
    "gwas.row.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a84fa-8071-41c6-b0f4-4b82c02b8199",
   "metadata": {},
   "source": [
    "Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.\n",
    "\n",
    "Hail makes it easy to visualize results! Let’s make a Manhattan plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70e1d8-7857-499e-bef3-0ed8968beffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.manhattan(gwas.p_value)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5873b-cd39-4fc2-9b51-09fa09e924df",
   "metadata": {},
   "source": [
    "This doesn’t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ca913-3b03-4fd1-a0df-4f2a62692017",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.qq(gwas.p_value)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a0b77-dfa6-4fac-b34c-e4907fb03893",
   "metadata": {},
   "source": [
    "## Confounded!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a4335-b9ae-417a-a4b4-ed1f4c1d6514",
   "metadata": {},
   "source": [
    "The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.\n",
    "\n",
    "We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a **stratified distribution** of the phenotype. The solution is to include ancestry as a covariate in our regression.\n",
    "\n",
    "The **linear_regression_rows** function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.\n",
    "\n",
    "The **pca** function produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The **hwe_normalized_pca** function does the same, using HWE-normalized genotypes for the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b55f0-b8a8-490e-9399-b5d6bc66cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.GT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f1140-1810-4256-8051-ae3f84cba377",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e7012-b34a-4bfa-b40c-a74d5a6c1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468cb35-c59d-4390-bc8f-bfc4dba6a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs.show(5, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fccd0ff-fcf6-4629-bd89-dfc2cddf8beb",
   "metadata": {},
   "source": [
    "Now that we’ve got principal components per sample, we may as well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0083fa0e-a2e6-4dc2-aa87-795728976dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_cols(scores = pcs[mt.s].scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1de97-4261-401e-9840-6639ee8d823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.scatter(mt.scores[0],\n",
    "                    mt.scores[1],\n",
    "                    label=mt.pheno.SuperPopulation,\n",
    "                    title='PCA', xlabel='PC1', ylabel='PC2')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770b933-7160-4c0f-9d1d-e7f37c038758",
   "metadata": {},
   "source": [
    "Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1148a65-caf2-4481-951b-5651da095d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas = hl.linear_regression_rows(\n",
    "    y=mt.pheno.CaffeineConsumption,\n",
    "    x=mt.GT.n_alt_alleles(),\n",
    "    covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac96bb-3333-412c-b5fd-d25ea1e70df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c00cd-3a5c-4f19-86c9-a8f93f4d59c4",
   "metadata": {},
   "source": [
    "We’ll first make a Q-Q plot to assess inflation…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46816505-170a-4305-9538-9e12b0d7faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.qq(gwas.p_value)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad3912-ccd1-4300-8feb-7a3915dc85b3",
   "metadata": {},
   "source": [
    "That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5fb11-a4bc-4cd1-bbf9-6e143718ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.manhattan(gwas.p_value)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceeeea8-422d-4521-bf0d-94f4923542ce",
   "metadata": {},
   "source": [
    "We have found a caffeine consumption locus!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc5222-fc1a-45bf-b3ef-bdbc56066834",
   "metadata": {},
   "source": [
    "## Rare variant analysis\n",
    "Here we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02172681-34a6-4d93-a934-2ac5ab2cc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = mt.entries()\n",
    "results = (entries.group_by(pop = entries.pheno.SuperPopulation, chromosome = entries.locus.contig)\n",
    "      .aggregate(n_het = hl.agg.count_where(entries.GT.is_het())))\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53daa8-2039-437b-b6b0-1690829bacaf",
   "metadata": {},
   "source": [
    "We use the **MatrixTable.entries** method to convert our matrix table to a table (with one row for each sample for each variant). In this representation, it is easy to aggregate over any fields we like, which is often the first step of rare variant analysis.\n",
    "\n",
    "What if we want to group by minor allele frequency bin and hair color, and calculate the mean GQ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079fa608-7e44-427e-9d80-7f5d0de15e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = entries.annotate(maf_bin = hl.if_else(entries.info.AF[0]<0.01, \"< 1%\",\n",
    "                             hl.if_else(entries.info.AF[0]<0.05, \"1%-5%\", \">5%\")))\n",
    "\n",
    "results2 = (entries.group_by(af_bin = entries.maf_bin, purple_hair = entries.pheno.PurpleHair)\n",
    "      .aggregate(mean_gq = hl.agg.stats(entries.GQ).mean,\n",
    "                 mean_dp = hl.agg.stats(entries.DP).mean))\n",
    "\n",
    "results2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052d363-0511-4d95-9180-797976825d09",
   "metadata": {},
   "source": [
    "We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:\n",
    "\n",
    "Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint\n",
    "\n",
    "Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
